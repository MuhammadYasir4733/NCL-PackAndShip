{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9bede01-a7a9-40b4-8e31-1daa69a7e472",
   "metadata": {},
   "source": [
    "## Import necessary modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cb125ed-893e-475d-b3c3-d156d11d9fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import lit, sum,col, trim\n",
    "from pyspark.sql.window import Window "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3eaa616-b67a-4dd9-bc4a-1914314915bd",
   "metadata": {},
   "source": [
    "## Initialize SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c371d437-cdc1-4be1-8005-b8b6f862122c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Integration\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", \"/home/jovyan/drivers/mssql-jdbc-11.2.0.jre8.jar\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0835c416-b70d-4b31-b077-d5c46002e101",
   "metadata": {},
   "source": [
    "## Database connection properties for source and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e14b3cc8-04ea-4a97-862a-c812728449a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read configuration from config file\n",
    "with open('config.json', 'r') as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "# Set the environment (e.g., 'development' or 'production')\n",
    "environment = 'development'  \n",
    "\n",
    "# Extract connection properties based on the environment\n",
    "source_connection_properties = config[environment]['source_connection_properties']\n",
    "target_connection_properties = config[environment]['target_connection_properties']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c843399-17a5-49c8-872c-dfaf41b37135",
   "metadata": {},
   "source": [
    "## PODetail Integration\n",
    "* __Custom queries for PODetail's source and target tables__\n",
    "* __Load data from source and target PODetail's tables__\n",
    "* __Filter data__\n",
    "* __Apply necessary transformations__\n",
    "* __Write filtered data to target table__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "24cf5c1a-2e1f-4932-983f-91a4ef8c53fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in dataframe :  0\n",
      "No data to write to target table.\n"
     ]
    }
   ],
   "source": [
    "# Custom queries for source and target tables\n",
    "pod_source_query = \"\"\"\n",
    "    (\n",
    "    SELECT OrId, PO, StyleID, BuyMonth, ColorCode, ColorDesc,128 AS CreatedBy  \n",
    "    FROM [NCPMS].[dbo].[CutOrdSheetDtl]\n",
    "    GROUP BY OrId, PO, StyleID, BuyMonth, ColorCode, ColorDesc\n",
    "    ) AS temp\n",
    "\"\"\"\n",
    "\n",
    "pod_target_query = \"\"\"\n",
    "    (\n",
    "    SELECT DISTINCT OrId,PO,ColorCode FROM [Packing].[PODetail] \n",
    "    ) AS temp\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Load data from source and target tables\n",
    "pod_source_df = spark.read.jdbc(url=source_connection_properties['url'],\n",
    "                            table=pod_source_query,\n",
    "                            properties=source_connection_properties)\n",
    "\n",
    "pod_target_df = spark.read.jdbc(url=target_connection_properties['url'],\n",
    "                            table=pod_target_query,\n",
    "                            properties=target_connection_properties)\n",
    "\n",
    "\n",
    "# Filter records from source_df that do not exist in target table\n",
    "pod_filtered_df = pod_source_df.join(pod_target_df, \n",
    "                                     (pod_source_df[\"OrId\"] == pod_target_df[\"OrId\"]) &\n",
    "                                     (pod_source_df[\"PO\"] == pod_target_df[\"PO\"]) &\n",
    "                                     (pod_source_df[\"ColorCode\"] == pod_target_df[\"ColorCode\"]),\n",
    "                                     \"left_anti\").orderBy(\"OrId\", \"PO\", \"ColorCode\")\n",
    "\n",
    "# Apply necessary transformations to dataframe\n",
    "\n",
    "\n",
    "# Get number of records\n",
    "print(f\"Number of records in dataframe :  {pod_filtered_df.count()}\")\n",
    "\n",
    "# # Limit number of records\n",
    "# pod_filtered_df=pod_filtered_df.limit(10)\n",
    "\n",
    "\n",
    "# Check if transformed_df is not empty\n",
    "if not pod_filtered_df.isEmpty():\n",
    "    # Insert grouped records into target table\n",
    "    pod_filtered_df.write.jdbc(url=target_connection_properties['url'],\n",
    "                              table=\"[Packing].[PODetail]\",\n",
    "                              mode=\"append\",\n",
    "                              properties=target_connection_properties)\n",
    "else:\n",
    "    print(\"No data to write to target table.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbf0e8a-4d1d-4689-9043-eea8f55bd2d6",
   "metadata": {},
   "source": [
    "## UpcMappingDetail Integration\n",
    "* __Custom queries for UpcMappingDetail's source and target tables__\n",
    "* __Load data from source and target UpcMappingDetail's tables__\n",
    "* __Filter data__\n",
    "* __Apply necessary transformations__\n",
    "* __Write filtered data to target table__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "20f908b6-ad9c-4e61-b21c-dd224df297cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in dataframe :  0\n",
      "No data to write to target table.\n"
     ]
    }
   ],
   "source": [
    "# Custom queries for source and target tables\n",
    "upc_source_query = \"\"\"\n",
    "    (\n",
    "    SELECT po.PODetailID,cos.ItemSize,SUM(cos.OrderQty) as ReceivedQty,cos.DetailUPC,SUM(cos.OrderQty) as OrderQty,\n",
    "    MAX(cos.DescriptionDtl) as DescriptionDtl,MAX(cos.HTS) as HTS,128 AS CreatedBy\n",
    "    FROM [NCPMS].[dbo].[CutOrdSheetDtl] AS cos\n",
    "    JOIN [ActiveSooperWizerNCL].[Packing].[PODetail] AS po\n",
    "    ON cos.OrId = po.OrId AND cos.PO = po.PO AND cos.ColorCode = po.ColorCode\n",
    "    GROUP BY po.PODetailID,cos.ItemSize,cos.DetailUPC\n",
    "    ) AS temp\n",
    "\"\"\"\n",
    "\n",
    "upc_target_query = \"\"\"\n",
    "    (\n",
    "    SELECT DISTINCT PODetailID,ItemSize,DetailUPC FROM [Packing].[UpcMappingDetail] \n",
    "    ) AS temp\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Load data from source and target tables\n",
    "upc_source_df = spark.read.jdbc(url=source_connection_properties['url'],\n",
    "                            table=upc_source_query,\n",
    "                            properties=source_connection_properties)\n",
    "\n",
    "upc_target_df = spark.read.jdbc(url=target_connection_properties['url'],\n",
    "                            table=upc_target_query,\n",
    "                            properties=target_connection_properties)\n",
    "\n",
    "\n",
    "# Filter records from source_df that do not exist in target table\n",
    "upc_filtered_df = upc_source_df.join(upc_target_df, \n",
    "                                     (upc_source_df[\"PODetailID\"] == upc_target_df[\"PODetailID\"]) &\n",
    "                                     (upc_source_df[\"ItemSize\"] == upc_target_df[\"ItemSize\"]) &\n",
    "                                     (upc_source_df[\"DetailUPC\"] == upc_target_df[\"DetailUPC\"]),\n",
    "                                     \"left_anti\").orderBy(\"PODetailID\", \"ItemSize\")\n",
    "\n",
    "# Apply necessary transformations to dataframe\n",
    "\n",
    "\n",
    "# Get number of records\n",
    "print(f\"Number of records in dataframe :  {upc_filtered_df.count()}\")\n",
    "\n",
    "# # Limit number of records\n",
    "# upc_filtered_df=upc_filtered_df.limit(10)\n",
    "\n",
    "\n",
    "# Check if transformed_df is not empty\n",
    "if not upc_filtered_df.isEmpty():\n",
    "    # Insert grouped records into target table\n",
    "    upc_filtered_df.write.jdbc(url=target_connection_properties['url'],\n",
    "                              table=\"[Packing].[UpcMappingDetail]\",\n",
    "                              mode=\"append\",\n",
    "                              properties=target_connection_properties)\n",
    "else:\n",
    "    print(\"No data to write to target table.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74b5983-3d09-4186-b7fb-c59eb6949ab9",
   "metadata": {},
   "source": [
    "## ShipDetail Integration\n",
    "* __Custom queries for ShipDetail's source and target tables__\n",
    "* __Load data from source and target ShipDetail's tables__\n",
    "* __Filter data__\n",
    "* __Apply necessary transformations__\n",
    "* __Write filtered data to target table__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "17ad2b59-e0c2-437a-8fa1-7d5f0b0cc2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in dataframe :  0\n",
      "No data to write to target table.\n"
     ]
    }
   ],
   "source": [
    "# Custom queries for source and target tables\n",
    "shipdtl_source_query = \"\"\"\n",
    "    (\n",
    "    SELECT po.PODetailID, cos.ExFactoryDate,cos.ShipTo,cos.TransportMethod,cos.Col2 AS StoreDetail,cos.ShipType,cos.PackingDtl,128 AS CreatedBy\n",
    "    FROM [NCPMS].[dbo].[CutOrdSheetDtl] AS cos\n",
    "    JOIN [ActiveSooperWizerNCL].[Packing].[PODetail] AS po ON cos.OrId = po.OrId AND cos.PO = po.PO AND cos.ColorCode = po.ColorCode\n",
    "    GROUP BY po.PODetailID, cos.ExFactoryDate, cos.ShipTo, cos.TransportMethod, cos.Col2,cos.ShipType,cos.PackingDtl\n",
    "    ) AS temp\n",
    "\"\"\"\n",
    "\n",
    "shipdtl_target_query = \"\"\"\n",
    "    (\n",
    "    SELECT DISTINCT PODetailID,ExFactoryDate,ShipTo,TransportMethod,StoreDetail,ShipType,PackingDtl FROM [Packing].[ShipDetail] \n",
    "    ) AS temp\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Load data from source and target tables\n",
    "shipdtl_source_df = spark.read.jdbc(url=source_connection_properties['url'],\n",
    "                            table=shipdtl_source_query,\n",
    "                            properties=source_connection_properties)\n",
    "\n",
    "shipdtl_target_df = spark.read.jdbc(url=target_connection_properties['url'],\n",
    "                            table=shipdtl_target_query,\n",
    "                            properties=target_connection_properties)\n",
    "\n",
    "\n",
    "# Filter records from source_df that do not exist in target table\n",
    "shipdtl_filtered_df = shipdtl_source_df.join(shipdtl_target_df, \n",
    "                                        (shipdtl_source_df[\"PODetailID\"] == shipdtl_target_df[\"PODetailID\"]) &\n",
    "                                        (shipdtl_source_df[\"ExFactoryDate\"] == shipdtl_target_df[\"ExFactoryDate\"]) &\n",
    "                                        (shipdtl_source_df[\"ShipTo\"] == shipdtl_target_df[\"ShipTo\"]) &\n",
    "                                        (shipdtl_source_df[\"TransportMethod\"] == shipdtl_target_df[\"TransportMethod\"]) &\n",
    "                                        (shipdtl_source_df[\"StoreDetail\"] == shipdtl_target_df[\"StoreDetail\"]) & \n",
    "                                        (shipdtl_source_df[\"ShipType\"] == shipdtl_target_df[\"ShipType\"]) &   \n",
    "                                        (shipdtl_source_df[\"PackingDtl\"] == shipdtl_target_df[\"PackingDtl\"]),\n",
    "                                        \"left_anti\").orderBy(\"PODetailID\")\n",
    "\n",
    "# Apply necessary transformations to dataframe\n",
    "\n",
    "\n",
    "# Get number of records\n",
    "print(f\"Number of records in dataframe :  {shipdtl_filtered_df.count()}\")\n",
    "\n",
    "# # Limit number of records\n",
    "# shipdtl_filtered_df=shipdtl_filtered_df.limit(10)\n",
    "\n",
    "\n",
    "# Check if transformed_df is not empty\n",
    "if not shipdtl_filtered_df.isEmpty():\n",
    "    # Insert grouped records into target table\n",
    "    shipdtl_filtered_df.write.jdbc(url=target_connection_properties['url'],\n",
    "                              table=\"[Packing].[ShipDetail]\",\n",
    "                              mode=\"append\",\n",
    "                              properties=target_connection_properties)\n",
    "else:\n",
    "    print(\"No data to write to target table.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e57d7e-a2d7-47ba-9db4-fc20598429f8",
   "metadata": {},
   "source": [
    "## PackAndShipDetail Integration\n",
    "* __Custom queries for PackAndShipDetail's source and target tables__\n",
    "* __Load data from source and target PackAndShipDetail's tables__\n",
    "* __Filter data__\n",
    "* __Apply necessary transformations__\n",
    "* __Write filtered data to target table__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4437c7e6-fb6f-4650-ac29-c94294e18a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in dataframe :  0\n",
      "No data to write to target table.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Custom queries for source and target tables\n",
    "PackAndshipdtl_source_query = \"\"\"\n",
    "    (\n",
    "    SELECT upc.UpcMappingDetailID, shd.ShipDetailID, cos.Kit, 1 AS Locked, cos.PackRatio, cos.PackType, SUM(cos.OrderQty) AS OrderQty,\n",
    "    SUM(cos.UDT) AS UDT, SUM(cos.ODT) AS ODT, 'N/A' AS Ext ,Max(cos.CLength) AS CLength ,Max(cos.CWidth) AS CWidth ,Max(cos.CHeight) AS CHeight,128 AS CreatedBy\n",
    "    FROM [NCPMS].[dbo].[CutOrdSheetDtl] AS cos\n",
    "    JOIN [ActiveSooperWizerNCL].[Packing].[PODetail] AS po ON cos.OrId = po.OrId AND cos.PO = po.PO And cos.ColorCode = po.ColorCode\n",
    "    JOIN [ActiveSooperWizerNCL].[Packing].[UpcMappingDetail] AS upc ON po.PODetailID = upc.PODetailID AND cos.ItemSize = upc.ItemSize AND cos.DetailUPC = upc.DetailUPC \n",
    "    JOIN [ActiveSooperWizerNCL].[Packing].[ShipDetail] AS shd ON cos.ExFactoryDate = shd.ExFactoryDate AND cos.ShipTo = shd.ShipTo AND cos.TransportMethod = shd.TransportMethod\n",
    "    AND cos.Col2 = shd.StoreDetail AND cos.ShipType = shd.ShipType AND cos.PackingDtl = shd.PackingDtl AND po.PODetailID = shd.PODetailID\n",
    "    GROUP BY upc.UpcMappingDetailID,shd.ShipDetailID,cos.Kit,cos.PackRatio,cos.PackType,cos.ExFactoryDate,cos.ShipTo,\n",
    "    cos.TransportMethod,cos.Col2,cos.ShipType,cos.PackingDtl\n",
    "    ) AS temp\n",
    "\"\"\"\n",
    "\n",
    "PackAndshipdtl_target_query = \"\"\"\n",
    "    (\n",
    "    SELECT DISTINCT UpcMappingDetailID,ShipDetailID,Kit,PackRatio,PackType FROM [Packing].[PackAndShipDetail] \n",
    "    ) AS temp\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Load data from source and target tables\n",
    "PackAndshipdtl_source_df = spark.read.jdbc(url=source_connection_properties['url'],\n",
    "                            table=PackAndshipdtl_source_query,\n",
    "                            properties=source_connection_properties)\n",
    "\n",
    "PackAndshipdtl_target_df = spark.read.jdbc(url=target_connection_properties['url'],\n",
    "                            table=PackAndshipdtl_target_query,\n",
    "                            properties=target_connection_properties)\n",
    "\n",
    "\n",
    "# Filter records from source_df that do not exist in target table\n",
    "PackAndshipdtl_filtered_df = PackAndshipdtl_source_df.join(PackAndshipdtl_target_df, \n",
    "                                        (PackAndshipdtl_source_df[\"UpcMappingDetailID\"] == PackAndshipdtl_target_df[\"UpcMappingDetailID\"]) &\n",
    "                                        (PackAndshipdtl_source_df[\"ShipDetailID\"] == PackAndshipdtl_target_df[\"ShipDetailID\"]) &\n",
    "                                        (PackAndshipdtl_source_df[\"Kit\"] == PackAndshipdtl_target_df[\"Kit\"]) &\n",
    "                                        (PackAndshipdtl_source_df[\"PackRatio\"] == PackAndshipdtl_target_df[\"PackRatio\"]) &\n",
    "                                        (PackAndshipdtl_source_df[\"PackType\"] == PackAndshipdtl_target_df[\"PackType\"]),\n",
    "                                        \"left_anti\").orderBy(\"ShipDetailID\",\"UpcMappingDetailID\")\n",
    "\n",
    "# Apply necessary transformations to dataframe\n",
    "\n",
    "\n",
    "# Get number of records\n",
    "print(f\"Number of records in dataframe :  {PackAndshipdtl_filtered_df.count()}\")\n",
    "\n",
    "# # Limit number of records\n",
    "# PackAndshipdtl_filtered_df=PackAndshipdtl_filtered_df.limit(10)\n",
    "\n",
    "\n",
    "# Check if transformed_df is not empty\n",
    "if not PackAndshipdtl_filtered_df.isEmpty():\n",
    "    # Insert grouped records into target table\n",
    "    PackAndshipdtl_filtered_df.write.jdbc(url=target_connection_properties['url'],\n",
    "                              table=\"[Packing].[PackAndShipDetail]\",\n",
    "                              mode=\"append\",\n",
    "                              properties=target_connection_properties)\n",
    "else:\n",
    "    print(\"No data to write to target table.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc271a2f-7bbd-491d-a5c8-05db40a1db64",
   "metadata": {},
   "source": [
    "## For Testing Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d4a9aa15-4a11-43a7-9744-b940027e8445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define database connection properties for source and target\n",
    "# source_connection_properties = {\n",
    "#     \"url\": \"jdbc:sqlserver://10.0.0.9:1435;databaseName=NCPMS;encrypt=false\",\n",
    "#     \"user\": \"sa\",\n",
    "#     \"password\": \"spts@3311\",\n",
    "#     \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "# }\n",
    "\n",
    "# target_connection_properties = {\n",
    "#     \"url\": \"jdbc:sqlserver://10.0.0.9:1435;databaseName=NCLSooperWizerQA;encrypt=false\",\n",
    "#     \"user\": \"sa\",\n",
    "#     \"password\": \"spts@3311\",\n",
    "#     \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "# }\n",
    "\n",
    "# # Custom queries for source and target tables\n",
    "# order_source_query = \"\"\"\n",
    "#     (\n",
    "#     SELECT OrId AS SaleOrderCode, Buyer AS Customer, CompanyId, BuyMonth, StyleNo,OrId \n",
    "#     FROM [dbo].[uniqueBundleWise_vw] \n",
    "#     WHERE CompanyId IN ('CW', 'NCL', 'PCI', 'FIN')\n",
    "#     ) AS temp\n",
    "# \"\"\"\n",
    "\n",
    "# order_target_query = \"\"\"\n",
    "#     (\n",
    "#     SELECT DISTINCT SaleOrderCode \n",
    "#     FROM [Essentials].[SaleOrder]\n",
    "#     ) AS temp\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# # Load data from source and target tables\n",
    "# order_source_df = spark.read.jdbc(url=source_connection_properties['url'],\n",
    "#                             table=order_source_query,\n",
    "#                             properties=source_connection_properties)\n",
    "\n",
    "# order_target_df = spark.read.jdbc(url=target_connection_properties['url'],\n",
    "#                             table=order_target_query,\n",
    "#                             properties=target_connection_properties)\n",
    "\n",
    "# order_source_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c4781a-8f5d-40e2-be04-909571f67d83",
   "metadata": {},
   "source": [
    "## Stop SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33f88b0f-e88f-45aa-a8ae-888b14493974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddde213-a41f-4c6e-982b-ce2c2987a0d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
